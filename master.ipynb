{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDS 560 SpotOnResponse Srping 2024 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are codes for various different steps of our project. All codes related to GUI deployment are under the GUI folder in .py files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sponor Data Processing\n",
    "\n",
    "This section includes materials on combining the csv/xlsx files provided by the sponsor into a single dataframe for ease of eda/modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the collection of file provided and get the folder path. We chose to use colab for all processing\n",
    "folder_path = '/content/drive/My Drive/IDS560-SpotOnResponse/disaster_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used the below functions to see which of jims tables had the same schema\n",
    "#I then organized these tables into separate folders in Drive\n",
    "\n",
    "# Function to check file extension and read file accordingly\n",
    "def read_file(file_path):\n",
    "    if file_path.endswith('.xlsx'):\n",
    "        return pd.read_excel(file_path)\n",
    "    elif file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path)\n",
    "    else:\n",
    "        print(f\"Unsupported file format: {file_path}\")\n",
    "        return None\n",
    "\n",
    "# List all files in the folder\n",
    "def list_files(start_path):\n",
    "  counter = 0 #used to count the amount of files\n",
    "  sameSchema = 0 #experiment to see how different schemas are\n",
    "  \"\"\"Recursively lists all files under the given start path.\"\"\"\n",
    "  for root, dirs, files in os.walk(start_path):\n",
    "      path = root.split(os.sep)\n",
    "      for file in files:\n",
    "          print(f\"{'/'.join(path)}/{file}\")\n",
    "          counter += 1\n",
    "          current_df = read_file(f\"{'/'.join(path)}/{file}\")\n",
    "          if (set(current_df.dtypes.index) == set(sample_xlsx2.dtypes.index)): #checking files againts a table called 'sample_xlsx2'.\n",
    "            sameSchema += 1\n",
    "          else:\n",
    "            print(\"The above file has a different schema\")\n",
    "\n",
    "  print(\"total files:\", counter)\n",
    "  print(\"percent same as sample:\", sameSchema / counter)\n",
    "\n",
    "#list_files(folder_path + '2022 Events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BEWARE OF RUNNING THIS IF YOU ARE BETWEEN combine_file RUNS\n",
    "all_dataframes = []  # List to store individual dataframes for code below\n",
    "\n",
    "#This function will be used to combine tables with common schema into one dataframe\n",
    "def combine_files(folder_path):\n",
    "    # Walk through the folder and process each file\n",
    "    counter = 0\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check for .csv or .xlsx extension\n",
    "            if file.endswith('.csv') or file.endswith('.xlsx'):\n",
    "                full_file_path = os.path.join(root, file)\n",
    "                df = read_file(full_file_path)\n",
    "\n",
    "                if df is not None:\n",
    "                    # Prompt for additional information\n",
    "                    disaster_name = input(f\"Enter disaster name for {file}: \")\n",
    "                    day_of_week = input(f\"Enter day of the week for {file}: \") #monday, firday, etc.\n",
    "                    day_of_disaster = input(f\"Enter day of disaster for {file}: \") #how many days since storm hit?\n",
    "                    year = input(f\"Enter year of disaster for {file}: \")\n",
    "                    input_date = input(f\"Enter date of disaster for {file}: \")\n",
    "\n",
    "                    df['Disaster Name'] = disaster_name\n",
    "                    df['Day of Week'] = day_of_week\n",
    "                    df['Day of Disaster'] = day_of_disaster\n",
    "                    df['Year'] = year\n",
    "                    df['Date'] = input_date\n",
    "\n",
    "                    if (counter == 0 and len(all_dataframes) > 0):\n",
    "                      userBoolean = True\n",
    "                      while (userBoolean):\n",
    "                        userString = input(\"there are currently dataframes in shared list. stop program or enter 'yes' to continue:\")\n",
    "                        if userString == 'yes':\n",
    "                          userBoolean = False\n",
    "                          counter += 1\n",
    "                    all_dataframes.append(df)\n",
    "                    counter += 1\n",
    "\n",
    "    # Combine all dataframes into one\n",
    "    combined_dataframe = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "    return combined_dataframe\n",
    "\n",
    "#Below is code we ran to combine files with the same schema. Recommend saving these to file directiry\n",
    "df_2020_HWTX = combine_files('/content/drive/My Drive/IDS560-SpotOnResponse/same_schema2')\n",
    "df_winter = combine_files('/content/drive/My Drive/IDS560-SpotOnResponse/2022 Winter Storm/')\n",
    "df_2022 = combine_files('/content/drive/My Drive/IDS560-SpotOnResponse/2022 Hurricanes/')\n",
    "df_2023_events = combine_files('/content/drive/My Drive/IDS560-SpotOnResponse/2023 Events/')\n",
    "claud_elsa = combine_files('/content/drive/My Drive/IDS560-SpotOnResponse/Claudette/')\n",
    "df_2021 = combine_files('/content/drive/My Drive/IDS560-SpotOnResponse/same_schema3/')\n",
    "df_2020 = combine_files('/content/drive/My Drive/IDS560-SpotOnResponse/same_schema/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we combine all files\n",
    "\n",
    "#df_2020_HWTX = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/same_schema2/HW_TX.csv')\n",
    "#df_winter = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/2022 Winter Storm/winter.csv')\n",
    "#df_2022 = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/2022 Hurricanes/hurricane_2022.csv')\n",
    "#df_2023_events = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/2023 Events/idalia.csv')\n",
    "#claud_elsa = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/Claudette/claud_elsa.csv')\n",
    "#df_2021 = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/same_schema3/hurricanes_2021.csv')\n",
    "#df_2020 = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/same_schema/hurricanes_2020.csv')\n",
    "\n",
    "#combining file here\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "#initialize final_df fields with arbitrary starting df selection\n",
    "final_df['Date'] = df_2020['Date']\n",
    "final_df['Disaster Name'] = df_2020['Disaster Name']\n",
    "final_df['Day of Week'] = df_2020['Day of Week']\n",
    "final_df['Day of Disaster'] = df_2020['Day of Disaster']\n",
    "final_df['Year'] = df_2020['Year']\n",
    "final_df['PID'] = df_2020['PID']\n",
    "final_df['full_location'] = df_2020['company_address2']\n",
    "final_df['LON'] = df_2020['LON']\n",
    "final_df['LAT'] = df_2020['LAT']\n",
    "final_df['COMPANY_NAME'] = df_2020['COMPANY_NAME']\n",
    "final_df['CATEGORY1'] = df_2020['CATEGORY1']\n",
    "final_df['StoreType'] = df_2020['StoreType']\n",
    "final_df['RATING'] = df_2020['RATING']\n",
    "final_df['REVIEW_NUMBER'] = df_2020['REVIEW_NUMBER']\n",
    "final_df['WT1'] = df_2020['WT1']\n",
    "final_df['WT2'] = df_2020['WT2']\n",
    "final_df['WT3'] = df_2020['WT3']\n",
    "final_df['WT4'] = df_2020['WT4']\n",
    "final_df['WT5'] = df_2020['WT5']\n",
    "final_df['WT6'] = df_2020['WT6']\n",
    "final_df['WT7'] = df_2020['WT7']\n",
    "final_df['pics2'] = df_2020['pics2']\n",
    "final_df['verified'] = df_2020['verified']\n",
    "final_df['tempClosed'] = df_2020['tempClosed']\n",
    "\n",
    "#For texas hawaii df\n",
    "df_2020_HWTX = df_2020_HWTX.rename(columns={'company_address2' : 'full_location'})\n",
    "selected_columns = df_2020_HWTX[['Date', 'Disaster Name', 'Year', 'full_location', 'LON', 'LAT', 'COMPANY_NAME', 'CATEGORY1', 'StoreType', 'RATING', 'REVIEW_NUMBER', 'WT1', 'WT2', 'WT3', 'WT4', 'WT5', 'WT6', 'WT7', 'pics2', 'verified', 'tempClosed', 'Day of Week', 'Day of Disaster', 'PID']]\n",
    "\n",
    "final_df = pd.concat([final_df, selected_columns], ignore_index=True)\n",
    "\n",
    "#for winter storm\n",
    "#hours are pipe delimited in workTime, need to address this\n",
    "df_winter = df_winter.rename(columns={'company_address2' : 'full_location'})\n",
    "pipe_dfs = df_winter[['Date', 'Disaster Name', 'Year', 'Day of Week', 'Day of Disaster', 'PID', 'full_location', 'LON', 'LAT', 'COMPANY_NAME', 'CATEGORY1', 'StoreType', 'RATING', 'REVIEW_NUMBER', 'workTime', 'pics2', 'verified', 'tempClosed']]\n",
    "\n",
    "#for df 2022\n",
    "#just like winter storm the hours are pipe delimited in workTime, need to address this\n",
    "df_2022 = df_2022.rename(columns={'company_address2' : 'full_location'})\n",
    "selected_columns = df_2022[['Date', 'Disaster Name', 'Year', 'Day of Week', 'Day of Disaster', 'PID', 'full_location', 'LON', 'LAT', 'COMPANY_NAME', 'CATEGORY1', 'StoreType', 'RATING', 'REVIEW_NUMBER', 'workTime', 'pics2', 'verified', 'tempClosed']]\n",
    "\n",
    "pipe_dfs = pd.concat([pipe_dfs, selected_columns], ignore_index=True)\n",
    "\n",
    "#for claud_elsa\n",
    "#just like winter storm the hours are pipe delimited in workTime, need to address this\n",
    "claud_elsa = claud_elsa.rename(columns={'company_address2' : 'full_location'})\n",
    "selected_columns = claud_elsa[['Date', 'Disaster Name', 'Year', 'Day of Week', 'Day of Disaster', 'PID', 'full_location', 'LON', 'LAT', 'COMPANY_NAME', 'CATEGORY1', 'StoreType', 'RATING', 'REVIEW_NUMBER', 'workTime', 'pics2', 'verified', 'tempClosed']]\n",
    "\n",
    "pipe_dfs = pd.concat([pipe_dfs, selected_columns], ignore_index=True)\n",
    "\n",
    "#For 2023 Events and 2021\n",
    "#Also pipe delimited\n",
    "df_2023_events = df_2023_events.rename(columns={'company_address2' : 'full_location'})\n",
    "selected_columns = df_2023_events[['Date', 'Disaster Name', 'Year', 'Day of Week', 'Day of Disaster', 'PID', 'full_location', 'LON', 'LAT', 'COMPANY_NAME', 'CATEGORY1', 'StoreType', 'RATING', 'REVIEW_NUMBER', 'workTime', 'pics2', 'verified', 'tempClosed']]\n",
    "\n",
    "pipe_dfs = pd.concat([pipe_dfs, selected_columns], ignore_index=True)\n",
    "\n",
    "df_2021 = df_2021.rename(columns={'company_address2' : 'full_location'})\n",
    "selected_columns = df_2021[['Date', 'Disaster Name', 'Year', 'Day of Week', 'Day of Disaster', 'PID', 'full_location', 'LON', 'LAT', 'COMPANY_NAME', 'CATEGORY1', 'StoreType', 'RATING', 'REVIEW_NUMBER', 'workTime', 'pics2', 'verified', 'tempClosed']]\n",
    "\n",
    "pipe_dfs = pd.concat([pipe_dfs, selected_columns], ignore_index=True)\n",
    "\n",
    "pipe_dfs = pipe_dfs.drop_duplicates()\n",
    "final_df = final_df.drop_duplicates()\n",
    "\n",
    "print(\"piped dfs length:\", len(pipe_dfs))\n",
    "print('2020 dfs length:', len(final_df))\n",
    "print('combined length:', len(pipe_dfs) + len(final_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sponsor's historical files have store hours broken down into a column for each day\n",
    "#Thew sponsor's later files have them pipe delimited in one column.\n",
    "\n",
    "def parse_store_hours(row):\n",
    "    if pd.isnull(row):\n",
    "        return [None] * 7  # Return a list of None values if the row is missing\n",
    "    hours_by_day = {str(i): None for i in range(1, 8)}  # Initialize days with None values\n",
    "    for part in row.split('|'):\n",
    "        # Check if ':' is in the part, indicating a day:hours format\n",
    "        if ':' in part:\n",
    "            day, text = part.split(': ', 1)  # Split on the first occurrence of ': '\n",
    "            hours_by_day[day] = part  # Store the entire part, e.g., \"1: 10AMâ€“8PM\"\n",
    "        else:\n",
    "            # If ':' is not present, assume the entire part is to be used as is\n",
    "            # It could be just a day number or some other format; treated as an entire record\n",
    "            hours_by_day[part] = part  # The key and value are the same because the format is undetermined\n",
    "    return [hours_by_day[str(i)] for i in range(1, 8)]  # Return a list of records for each day\n",
    "\n",
    "pipe_dfs[['WT1', 'WT2', 'WT3', 'WT4', 'WT5', 'WT6', 'WT7']] = pd.DataFrame(pipe_dfs['workTime'].apply(parse_store_hours).tolist(), index=pipe_dfs.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = pipe_dfs[['Date', 'Disaster Name', 'Year', 'Day of Week', 'Day of Disaster', 'PID',\n",
    "       'full_location', 'LON', 'LAT', 'COMPANY_NAME', 'CATEGORY1', 'StoreType',\n",
    "       'RATING', 'REVIEW_NUMBER', 'pics2', 'verified',\n",
    "       'tempClosed', 'WT1', 'WT2', 'WT3', 'WT4', 'WT5', 'WT6', 'WT7']]\n",
    "\n",
    "#All dataframes are combined. Save this!\n",
    "final_df = pd.concat([final_df, selected_columns], ignore_index=True)\n",
    "\n",
    "file_path = '/content/drive/My Drive/IDS560-SpotOnResponse/combined_google_data.csv'\n",
    "\n",
    "final_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Data Collection/Processing\n",
    "\n",
    "This section includes syntax for how data was collected from the census as well as how it is processed to be attached to the sponsor's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import difflib #this will be used for fuzzy joins\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below are different functions for querying various census datasets\n",
    "#Please refer to Census API documentation for more details\n",
    "#API key is easily obtained from census website\n",
    "\n",
    "def get_ABS_data(api_key, state_code):\n",
    "  #sales, value of shipments, revenue\n",
    "    url = f\"https://api.census.gov/data/2017/abscs?get=NAME,GEO_ID,NAICS2017_LABEL,FIRMPDEMP,EMP,PAYANN,RCPPDEMP,&for=place:*&in=state:{state_code}&NAICS2017=*&key={api_key}\"\n",
    "\n",
    "    # Make a GET request to the API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_BP_data(api_key, state_code=None, zip=None):\n",
    "    county_url = f\"https://api.census.gov/data/2021/cbp?get=ESTAB,LFO,NAICS2017_LABEL,NAME,EMP,EMPSZES,ESTAB,INDGROUP,INDLEVEL&for=state:{state_code}&NAICS2017=*&key={api_key}\"\n",
    "    zip_url = f\"https://api.census.gov/data/2018/zbp?get=EMP,EMPSZES,ESTAB,PAYANN,INDGROUP,INDLEVEL,PAYANN,PAYQTR1,SECTOR,SUBSECTOR&for=zipcode:{zip}&NAICS2017=*&key={api_key}\"\n",
    "\n",
    "    if zip != None:\n",
    "      url = zip_url\n",
    "    else:\n",
    "      url = county_url\n",
    "\n",
    "    # Make a GET request to the API\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the JSON response\n",
    "        return response.json()\n",
    "\n",
    "def get_EC_data(api_key, state_code):\n",
    "  #RCP TOT: sales, value of shipment, or revenues\n",
    "  url = f\"https://api.census.gov/data/2017/ecnbasic?get=NAICS2017_LABEL,EMP,NAME,GEO_ID,ESTAB,FIRM,INDGROUP,INDLEVEL,OPEX,PAYANN,PAYQTR1,RCPTOT,SECTOR,SUBSECTOR,TAXSTAT&for=place:*&in=state:{state_code}&NAICS2017=*&key={api_key}\"\n",
    "\n",
    "  # Make a GET request to the API\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Check if the request was successful\n",
    "  if response.status_code == 200:\n",
    "      # Parse the JSON response\n",
    "      data = response.json()\n",
    "\n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#File from previous section\n",
    "file_path = \"/content/drive/MyDrive/IDS560-SpotOnResponse/combined_google_data.csv\"\n",
    "clean = pd.read_csv(file_path)\n",
    "\n",
    "#clean.head()\n",
    "\n",
    "#extracting location components\n",
    "clean[\"state\"] = clean['full_location'].str.extract(r',\\s([A-Z]{2})\\s')\n",
    "clean['zip_code'] = clean['full_location'].str.extract(r'(\\d{5})$')\n",
    "clean['county'] = clean['full_location'].str.extract(r'^(.*?),')\n",
    "unique_states = clean[\"state\"].unique()\n",
    "\n",
    "#print(unique_states)\n",
    "\n",
    "#Census encodes each state as a number so you need the below file\n",
    "state_code_path = \"/content/drive/MyDrive/IDS560-SpotOnResponse/state_codes.txt\"\n",
    "\n",
    "state_codes = pd.read_csv(state_code_path, delimiter='|')\n",
    "\n",
    "state_dict = {}\n",
    "\n",
    "for key, value in zip(state_codes['STUSAB'], state_codes['STATE']):\n",
    "  if (key in unique_states):\n",
    "    state_dict[key] = value\n",
    "\n",
    "state_dict['AL'] = '01'\n",
    "state_dict['AK'] = '02'\n",
    "state_dict['AZ'] = '04'\n",
    "state_dict['AR'] = '05'\n",
    "state_dict['CA'] = '06'\n",
    "state_dict['CO'] = '08'\n",
    "state_dict['CT'] = '09'\n",
    "del state_dict['VI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the code used for collection of census data\n",
    "\n",
    "#complete a preliminary request to get table schema from census\n",
    "econ_url = \"https://api.census.gov/data/2017/ecnbasic?get=NAICS2017_LABEL,EMP,NAME,GEO_ID,ESTAB,FIRM,INDGROUP,INDLEVEL,OPEX,PAYANN,PAYQTR1,RCPTOT,SECTOR,SUBSECTOR,TAXSTAT&for=place:*&in=state:01&NAICS2017=54&key=62879577960fb6e9ffe59583995ba0edeaa167cb\"\n",
    "\n",
    "# Make a GET request to the API\n",
    "response = requests.get(econ_url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "\n",
    "header = data[0]\n",
    "\n",
    "data = data[1:]\n",
    "\n",
    "c_df = pd.DataFrame(data, columns=header)\n",
    "c_df.head()\n",
    "\n",
    "#States that have the majority states with affected businesses\n",
    "states = {\"FL\" : 12, 'LA' : 22, 'TX' : 48, 'AL' : '01', 'MS' : 28, 'GA':13, 'SC':45, 'NC':37, 'NY':36}\n",
    "\n",
    "census_df = pd.DataFrame()\n",
    "census_df = c_df.iloc[0:0].copy()\n",
    "\n",
    "\n",
    "for k, v in state_dict.items():\n",
    "  print(k, v)\n",
    "  current_state = get_EC_data(api_key, str(v))\n",
    "\n",
    "  headers = current_state[0]\n",
    "  current_state = current_state[1:]\n",
    "\n",
    "  temp_df = pd.DataFrame(current_state, columns=headers)\n",
    "  print(\"State:\", k, \",records:\", len(temp_df))\n",
    "  census_df = census_df.append(temp_df, ignore_index=True)\n",
    "\n",
    "print(\"final df length:\", len(census_df))\n",
    "\n",
    "#SAVE census_df!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the [link](https://api.census.gov/data/2017/ecnbasic/variables.html) for the data dictionary for the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads in resulting csv from previous code cell\n",
    "census_df = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/census_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unique business categories from census data\n",
    "census_labels = census_df.groupby(\"NAICS2017_LABEL\").size().reset_index(name='Total_Counts').sort_values('Total_Counts', ascending = False)\n",
    "\n",
    "#Unique business categories from sponsor data\n",
    "google_labels = clean.groupby(\"CATEGORY1\").size().reset_index(name='Total_Counts').sort_values('Total_Counts', ascending = False)\n",
    "\n",
    "#We saved the two above tables to a csv and mannually mapped the most relevant census category to most of the google categories\n",
    "#We got a 'best guess' from running the below function to do a fuzzy mapping\n",
    "def fuzzy_join(col1, col2, top_n=3):\n",
    "    matches = []\n",
    "    for value in col1:\n",
    "        match = process.extract(value, col2, limit=top_n)\n",
    "        matches.extend([(value, m[0], m[1]) for m in match])\n",
    "    matches_df = pd.DataFrame(matches, columns=['Original', 'Match', 'Score'])\n",
    "    return matches_df\n",
    "\n",
    "#We took the below table and then did our manual mappings in excel\n",
    "top_matches = fuzzy_join(google_labels[\"CATEGORY1\"], census_labels[\"NAICS2017_LABEL\"], 5)\n",
    "\n",
    "\n",
    "#For very infrequent google categories, we imputed the category 'Retail trade'\n",
    "#The file is called new_census_google_mappings.csv\n",
    "file_path = '/content/drive/My Drive/IDS560-SpotOnResponse/new_census_google_mappings.csv'\n",
    "\n",
    "map_df = pd.read_csv(file_path)\n",
    "\n",
    "map_dict = dict(zip(map_df['Original'], map_df['Match']))\n",
    "\n",
    "#creates census category column using mapping table\n",
    "clean[\"Census_Category\"] = clean['CATEGORY1'].map(map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary with US state abbreviations as keys and state names as values\n",
    "us_states = {\n",
    "    \"AL\": \"Alabama\",\n",
    "    \"AK\": \"Alaska\",\n",
    "    \"AZ\": \"Arizona\",\n",
    "    \"AR\": \"Arkansas\",\n",
    "    \"CA\": \"California\",\n",
    "    \"CO\": \"Colorado\",\n",
    "    \"CT\": \"Connecticut\",\n",
    "    \"DE\": \"Delaware\",\n",
    "    \"FL\": \"Florida\",\n",
    "    \"GA\": \"Georgia\",\n",
    "    \"HI\": \"Hawaii\",\n",
    "    \"ID\": \"Idaho\",\n",
    "    \"IL\": \"Illinois\",\n",
    "    \"IN\": \"Indiana\",\n",
    "    \"IA\": \"Iowa\",\n",
    "    \"KS\": \"Kansas\",\n",
    "    \"KY\": \"Kentucky\",\n",
    "    \"LA\": \"Louisiana\",\n",
    "    \"ME\": \"Maine\",\n",
    "    \"MD\": \"Maryland\",\n",
    "    \"MA\": \"Massachusetts\",\n",
    "    \"MI\": \"Michigan\",\n",
    "    \"MN\": \"Minnesota\",\n",
    "    \"MS\": \"Mississippi\",\n",
    "    \"MO\": \"Missouri\",\n",
    "    \"MT\": \"Montana\",\n",
    "    \"NE\": \"Nebraska\",\n",
    "    \"NV\": \"Nevada\",\n",
    "    \"NH\": \"New Hampshire\",\n",
    "    \"NJ\": \"New Jersey\",\n",
    "    \"NM\": \"New Mexico\",\n",
    "    \"NY\": \"New York\",\n",
    "    \"NC\": \"North Carolina\",\n",
    "    \"ND\": \"North Dakota\",\n",
    "    \"OH\": \"Ohio\",\n",
    "    \"OK\": \"Oklahoma\",\n",
    "    \"OR\": \"Oregon\",\n",
    "    \"PA\": \"Pennsylvania\",\n",
    "    \"RI\": \"Rhode Island\",\n",
    "    \"SC\": \"South Carolina\",\n",
    "    \"SD\": \"South Dakota\",\n",
    "    \"TN\": \"Tennessee\",\n",
    "    \"TX\": \"Texas\",\n",
    "    \"UT\": \"Utah\",\n",
    "    \"VT\": \"Vermont\",\n",
    "    \"VA\": \"Virginia\",\n",
    "    \"WA\": \"Washington\",\n",
    "    \"WV\": \"West Virginia\",\n",
    "    \"WI\": \"Wisconsin\",\n",
    "    \"WY\": \"Wyoming\"\n",
    "}\n",
    "\n",
    "clean[\"full_state\"] = clean[\"state\"].map(us_states)\n",
    "\n",
    "#Need this column to do a fuzzy mapping of the google locations to census county locations\n",
    "clean[\"Place and State\"] = clean[\"county\"] + ', ' + clean[\"full_state\"]\n",
    "\n",
    "place_mappings = pd.DataFrame()\n",
    "\n",
    "#Run the below loop to get mapping table, similar to previous category one, for locations\n",
    "#This may take some time. We saved the resulting table to our file directory\n",
    "for s in census_df[\"State Name\"].unique():\n",
    "  print(\"current state:\", s)\n",
    "  #filter google data\n",
    "  current_google = clean[clean[\"full_state\"] == s].groupby(\"Place and State\").size().reset_index(name='Total_Counts').sort_values('Total_Counts', ascending = False)\n",
    "\n",
    "  #filter census data\n",
    "  current_census = census_df[census_df[\"State Name\"] == s].groupby(\"NAME\").size().reset_index(name='Total_Counts').sort_values('Total_Counts', ascending = False)\n",
    "\n",
    "  #fuzzy match on reduced schema\n",
    "  place_matches = fuzzy_join(current_google[\"Place and State\"], current_census[\"NAME\"], 1)\n",
    "\n",
    "  if (place_mappings.empty):\n",
    "    place_mappings = place_matches\n",
    "  else:\n",
    "    place_mappings = pd.concat([place_mappings, place_matches])\n",
    "\n",
    "place_mappings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads in resulting df from last section \n",
    "place_mappings = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/census_place_mappings.csv')\n",
    "\n",
    "#This will join the whole table with clean. We are intresting in the 'Match' column\n",
    "final_clean = pd.merge(clean, place_mappings, left_on=\"Place and State\", right_on=\"Original\", how='left')\n",
    "\n",
    "\n",
    "final_clean = final_clean.drop('Unnamed: 0', axis=1)\n",
    "final_clean = final_clean.drop('Score', axis=1)\n",
    "final_clean = final_clean.drop('Original', axis=1)\n",
    "\n",
    "final_clean = final_clean.rename(columns={'Match' : 'Census_Place'})\n",
    "\n",
    "#This is an important checkpoint to check for null values before performing final join\n",
    "joined_table = pd.merge(final_clean, census_df, left_on=['Census_Category', 'Census_Place'], right_on =['NAICS2017_LABEL', 'NAME'], how='left')\n",
    "joined_table = joined_table.drop(columns=['Unnamed: 0', 'NAICS2017_LABEL', 'NAME', 'GEO_ID','INDGROUP', 'INDLEVEL','SECTOR', 'SUBSECTOR', 'TAXSTAT', 'NAICS2017',\n",
    "       'state_y', 'place', 'MINLEVEL', 'State Name'])\n",
    "\n",
    "file_path = '/content/drive/My Drive/IDS560-SpotOnResponse/joined_data.csv'\n",
    "\n",
    "joined_table.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEMA SCORES joining\n",
    "\n",
    "This section includes the joing of the FEMA scores onto the google data and a discussion of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the FEMA scores provided by the fall 2023 semester\n",
    "index_path = '/content/drive/My Drive/IDS560-SpotOnResponse/final_clean_data.xlsx'\n",
    "zip_to_county = pd.read_excel(index_path, sheet_name='Sheet5', dtype={'zip': str}) #mapping table\n",
    "index_data = pd.read_excel(index_path, sheet_name='SVI') #Actual data\n",
    "\n",
    "#This is the df we created in the last section\n",
    "data_path = '/content/drive/My Drive/IDS560-SpotOnResponse/joined_data.csv'\n",
    "census_google_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracts county name\n",
    "index_data['county_map'] = index_data['Key'].str.split(',').str[0]\n",
    "\n",
    "#merges provided tables from fall 2023\n",
    "merged_index = pd.merge(zip_to_county, index_data, left_on=['state', 'county'], right_on=['STATEABBRV', 'county_map'], how='left')\n",
    "merged_index = merged_index.drop(columns=['state', 'county', 'Key', 'NRI_ID', 'STATE', 'STATEABBRV', 'COUNTY', 'COUNTYTYPE','county_map'])\n",
    "\n",
    "#clean zip code\n",
    "census_google_data['zip_code'] = census_google_data['zip_code'].apply(lambda x: str(int(x)) if pd.notnull(x) else x)\n",
    "\n",
    "final = pd.merge(census_google_data, merged_index, left_on='zip_code', right_on='zip', how='left')\n",
    "final = final.drop(columns=['zip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the true final csv containing many null attributes containing around 5.2 million records\n",
    "final.to_csv('/content/drive/My Drive/IDS560-SpotOnResponse/final.csv')\n",
    "\n",
    "#Now we start to create a clean subset for ourt analysis\n",
    "columns_to_inpute = ['pics2', 'verified', 'tempClosed']\n",
    "\n",
    "for column in columns_to_inpute:\n",
    "    final[column] = final[column].fillna(0)\n",
    "\n",
    "#This is the data used for our analysis\n",
    "#We have essentially filtered out records where the Census Join failed or the schedule is null\n",
    "prelim_data = final.dropna() #contains about 3.1 million records\n",
    "prelim_data.to_csv('/content/drive/My Drive/IDS560-SpotOnResponse/prelim_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA and Feature Engineering\n",
    "\n",
    "In this section we continue to perform some data processing, create a correlation ploy, and create derived columns to prepare dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, asin\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import DBSCAN\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in subset data from last section\n",
    "prelim_data = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/prelim_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There were additional duplicate records we found that are a result of a business being double counted\n",
    "#under multiple different categories and/or other arbitrary reasons\n",
    "#There were about 4000 such records and we removed them from our analysis\n",
    "# Group by the specified columns and count the occurrences\n",
    "grouped = prelim_data.groupby(['Date', 'Day of Disaster', 'Disaster Name', 'PID', 'COMPANY_NAME']).size()\n",
    "\n",
    "# Filter the grouped object to exclude counts that are 1\n",
    "filtered_grouped = grouped[grouped > 1]\n",
    "print(len(filtered_grouped))\n",
    "# Now you have a Series containing only the counts greater than 1\n",
    "# Extract the indices of the filtered groups\n",
    "indices_to_exclude = filtered_grouped.index\n",
    "\n",
    "# Filter the original dataset using the indices to exclude\n",
    "filtered_df = prelim_data[~prelim_data.set_index(['Date', 'Day of Disaster', 'Disaster Name', 'PID', 'COMPANY_NAME']).index.isin(indices_to_exclude)]\n",
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the createion of the 24 hour and 7 days a week columns\n",
    "columns_to_check = ['WT1', 'WT2', 'WT3', 'WT4', 'WT5', 'WT6', 'WT7']\n",
    "\n",
    "filtered_df['24_hours'] = filtered_df.apply(lambda row: any('24' in str(row[col]) for col in columns_to_check), axis=1).astype(int)\n",
    "\n",
    "for col in columns_to_check:\n",
    "    filtered_df[col] = (~filtered_df[col].isna()).astype(int)  # Convert non-NaN to 1 and NaN to 0\n",
    "\n",
    "filtered_df['7_days'] = (filtered_df[columns_to_check].sum(axis=1) == 7).astype(int)\n",
    "\n",
    "#The creation of teh distance metric inroduced by fall 23\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between two points\n",
    "    on the earth (specified in decimal degrees)\n",
    "    \"\"\"\n",
    "    # Convert decimal degrees to radians\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    r = 3956  # Radius of earth in miles. Use 6371 for kilometers\n",
    "    return c * r\n",
    "\n",
    "filtered_df['distance'] = np.nan\n",
    "# Get unique combinations of county and date\n",
    "for current_date in tqdm(filtered_df['Date'].unique()):\n",
    "    # Filter for temporarily closed stores on the current date\n",
    "    temp_closed_stores = filtered_df[(filtered_df['Date'] == current_date) & (filtered_df['tempClosed'] == 1)]\n",
    "\n",
    "    # Check if there are temporarily closed stores for the current date\n",
    "    if not temp_closed_stores.empty:\n",
    "        # Perform DBSCAN clustering\n",
    "        # Convert coordinates to radians for haversine_distances\n",
    "        coords = temp_closed_stores[['LAT', 'LON']].apply(np.radians).values\n",
    "        kms_per_radian = 6371.0088\n",
    "        epsilon = 1.609 / kms_per_radian  # 1.5km radius for neighborhood\n",
    "        db = DBSCAN(eps=epsilon, min_samples=5, metric='haversine').fit(coords)\n",
    "\n",
    "        # Calculate cluster centroids\n",
    "        cluster_labels = db.labels_\n",
    "        centroids = pd.DataFrame(columns=['LAT', 'LON'])\n",
    "        for cluster in set(cluster_labels):\n",
    "            if cluster != -1:  # Exclude noise points\n",
    "                members = coords[cluster_labels == cluster]\n",
    "                centroid = members.mean(axis=0)\n",
    "                centroids = centroids.append({'LAT': centroid[0], 'LON': centroid[1]}, ignore_index=True)\n",
    "\n",
    "        # Convert centroid radians back to degrees\n",
    "        centroids = centroids.apply(np.degrees)\n",
    "\n",
    "        # Calculate distances for all stores on the current date\n",
    "        all_stores = filtered_df[filtered_df['Date'] == current_date]\n",
    "        for idx, store in all_stores.iterrows():\n",
    "            distances = []\n",
    "            for _, centroid in centroids.iterrows():\n",
    "                distance = haversine(store['LON'], store['LAT'], centroid['LON'], centroid['LAT'])\n",
    "                distances.append(distance)\n",
    "\n",
    "            # Assign the minimum distance to the 'distance' column\n",
    "            if distances:\n",
    "                filtered_df.at[idx, 'distance'] = min(distances)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The creation of lagging variables for distance and tempClosed\n",
    "filtered_df['Date'] = pd.to_datetime(filtered_df['Date'])\n",
    "filtered_df = filtered_df.sort_values(by=['Disaster Name',   'PID', 'COMPANY_NAME', 'Day of Disaster', 'Date'])\n",
    "\n",
    "filtered_df = filtered_df.sort_values(by=['Disaster Name',   'PID', 'COMPANY_NAME', 'Day of Disaster', 'Date'])\n",
    "\n",
    "filtered_df = (filtered_df\n",
    "     .assign(\n",
    "        tempClosed_lag1 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['tempClosed'].shift(1),\n",
    "        tempClosed_lag2 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['tempClosed'].shift(2),\n",
    "        tempClosed_lag3 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['tempClosed'].shift(3),\n",
    "        tempClosed_lag4 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['tempClosed'].shift(4),\n",
    "        tempClosed_lag5 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['tempClosed'].shift(5),\n",
    "     )\n",
    ")\n",
    "\n",
    "filtered_df = filtered_df.sort_values(by=['Disaster Name',   'PID', 'COMPANY_NAME', 'Day of Disaster', 'Date'])\n",
    "\n",
    "filtered_df = (filtered_df\n",
    "     .assign(\n",
    "        distance_lag1 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['distance'].shift(1),\n",
    "        distance_lag2 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['distance'].shift(2),\n",
    "        distance_lag3 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['distance'].shift(3),\n",
    "        distance_lag4 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['distance'].shift(4),\n",
    "        distance_lag5 = lambda x: x.groupby(['Disaster Name',   'PID', 'COMPANY_NAME'])['distance'].shift(5),\n",
    "     )\n",
    ")\n",
    "\n",
    "#Save the resulting dataset\n",
    "filtered_df.to_csv('/content/drive/My Drive/IDS560-SpotOnResponse/prelim_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here are the packages for correlation matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/prelim_data.csv')\n",
    "\n",
    "df['tempClosed'] = df['tempClosed'].replace('1.0', 1)\n",
    "df['tempClosed'] = df['tempClosed'].replace('0.0', 0)\n",
    "df['tempClosed'] = df['tempClosed'].replace('0', 0)\n",
    "df['tempClosed'] = df['tempClosed'].replace('True', 1)\n",
    "\n",
    "numerical_features = ['EMP', 'ESTAB', 'FIRM', 'PAYANN', 'RCPTOT', 'RISK_SCORE', 'SOVI_SCORE', 'RESL_SCORE']\n",
    "\n",
    "# Compute and plot the correlation matrix\n",
    "correlation_matrix = df[numerical_features + ['tempClosed']].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "This section covers our code for modeling. Some additional columns may have entered the subsequent datasets in the previous code. Ensure that these are removed and that only the columns referenced in code are used.\n",
    "\n",
    "The section starts with teh creation of our GMM and ends with the logistic regression and random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how our GMM model was created\n",
    "df = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/prelim_data.csv')\n",
    "\n",
    "sel_cols = ['EMP','ESTAB','PAYANN', 'RCPTOT', 'RISK_SCORE','SOVI_SCORE','RESL_SCORE','24_hours','7_days', 'RATING', 'REVIEW_NUMBER', 'Census_Category']\n",
    "df2 = df.loc[:,sel_cols].drop_duplicates() #only want each business to be included once\n",
    "\n",
    "#Fill null values with 0\n",
    "df2.fillna(0,inplace=True)\n",
    "\n",
    "#Distribute categorical and numerical variables\n",
    "categorical_columns = ['Census_Category'] #should be approximately 100\n",
    "numerical_columns = [col for col in df2.columns if col not in categorical_columns]\n",
    "\n",
    "#Create pre-processor to encode categorical variables and scale numerical variables\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_columns),\n",
    "    ('cat', OneHotEncoder(), categorical_columns)\n",
    "])\n",
    "\n",
    "#Create transformed dataframe\n",
    "t_df2 = preprocessor.fit_transform(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We tested the performance of the GMM with different number of Gaussians and restrictions of covaiance matrices\n",
    "#The restrictions on the cms would indicate how much better the unrestricted cm would perform\n",
    "n_components = np.arange(1, 21)\n",
    "#spherical covariance\n",
    "s_models = []\n",
    "\n",
    "for n in n_components:\n",
    "    print(f\"Training GMM with {n} components...\")\n",
    "    model = GaussianMixture(n, covariance_type='spherical', random_state=42).fit(t_df2.toarray())\n",
    "    s_models.append(model)\n",
    "\n",
    "\n",
    "plt.plot(n_components, [m.bic(t_df2.toarray()) for m in s_models], label='BIC', linestyle='-', marker='o')\n",
    "plt.plot(n_components, [m.aic(t_df2.toarray()) for m in s_models], label='AIC', linestyle='--', marker='x')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components')\n",
    "plt.show()\n",
    "\n",
    "#Full covariance\n",
    "f_models = []\n",
    "\n",
    "for n in n_components:\n",
    "    print(f\"Training GMM with {n} components...\")\n",
    "    model = GaussianMixture(n, covariance_type='full', random_state=42).fit(t_df2.toarray())\n",
    "    f_models.append(model)\n",
    "\n",
    "plt.plot(n_components, [m.bic(t_df2.toarray()) for m in f_models], label='BIC', linestyle='-', marker='o')\n",
    "plt.plot(n_components, [m.aic(t_df2.toarray()) for m in f_models], label='AIC', linestyle='--', marker='x')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('n_components')\n",
    "plt.show()\n",
    "\n",
    "#The full covariance performed better and we chose 3 Gaussians\n",
    "model_with_3_gaussians = f_models[2]\n",
    "joblib.dump(model_with_3_gaussians, '/content/drive/My Drive/IDS560-SpotOnResponse/gmm_3_components.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modeling packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import joblib\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, recall_score, roc_auc_score, confusion_matrix, make_scorer, precision_recall_curve, roc_curve\n",
    "from sklearn.base import clone\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in clean subset of data\n",
    "Data_final = pd.read_csv('/content/drive/My Drive/IDS560-SpotOnResponse/prelim_data.csv')\n",
    "\n",
    "#extracts numbers from strings in pics2\n",
    "def extract_number(s):\n",
    "    match = re.search(r'\\d+', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "Data_final['pics2'] = Data_final['pics2'].apply(extract_number)\n",
    "\n",
    "#Strange error when reading in first temp closed lagging variable\n",
    "Data_final['tempClosed_lag1'] = pd.to_numeric(Data_final['tempClosed_lag1'], errors='coerce')\n",
    "\n",
    "Data_final['tempClosed_lag1'] = Data_final['tempClosed_lag1'].apply(lambda x: 1.0 if x else 0.0)\n",
    "\n",
    "#The distance columns should be the only columns at thispoint that have null values\n",
    "#We imputed an arbitrarily high number to essentially say that the business was very far from any affected area\n",
    "Data_final[\"distance\"] = Data_final[\"distance\"].fillna(1000)\n",
    "Data_final[\"distance_lag1\"] = Data_final[\"distance_lag1\"].fillna(1000) #might just keep this one and get rid of rest due to amount of nulls in other distance lag columns\n",
    "Data_final[\"distance_lag2\"] = Data_final[\"distance_lag2\"].fillna(1000)\n",
    "Data_final[\"distance_lag3\"] = Data_final[\"distance_lag3\"].fillna(1000)\n",
    "Data_final[\"distance_lag4\"] = Data_final[\"distance_lag4\"].fillna(1000)\n",
    "Data_final[\"distance_lag5\"] = Data_final[\"distance_lag5\"].fillna(1000)\n",
    "\n",
    "#Cleaning target variable\n",
    "Data_final['tempClosed'] = Data_final['tempClosed'].replace('1.0', 1)\n",
    "Data_final['tempClosed'] = Data_final['tempClosed'].replace('0.0', 0)\n",
    "Data_final['tempClosed'] = Data_final['tempClosed'].replace('0', 0)\n",
    "Data_final['tempClosed'] = Data_final['tempClosed'].replace('True', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting probability of belonging to each Gaussian\n",
    "#Load the model from colab\n",
    "gmm3 = joblib.load('/content/drive/My Drive/IDS560-SpotOnResponse/gmm_3_components.joblib')\n",
    "\n",
    "gmm_features = ['EMP','ESTAB','PAYANN', 'RCPTOT', 'RISK_SCORE','SOVI_SCORE','RESL_SCORE','24_hours','7_days', 'RATING', 'REVIEW_NUMBER', 'Census_Category']\n",
    "gmm_df = Data_final.loc[:,gmm_features]\n",
    "\n",
    "categorical_columns = ['Census_Category'] #should be approximately 100\n",
    "numerical_columns = [col for col in gmm_df.columns if col not in categorical_columns]\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_columns),\n",
    "    ('cat', OneHotEncoder(), categorical_columns)\n",
    "])\n",
    "gmm_df = preprocessor.fit_transform(gmm_df)\n",
    "\n",
    "probs = gmm3.predict_proba(gmm_df.toarray())\n",
    "prob_df = pd.DataFrame(probs, columns=['Gaussian_1', 'Gaussian_2', 'Gaussian_3'])\n",
    "\n",
    "Data_final = pd.concat([Data_final, prob_df], axis=1)\n",
    "\n",
    "X_df = Data_final[['Day of Week', 'Day of Disaster', 'RISK_SCORE', 'SOVI_SCORE', 'RESL_SCORE', 'EMP', 'ESTAB', 'PAYANN', 'RATING', 'RCPTOT', 'REVIEW_NUMBER', 'pics2',\n",
    "        'tempClosed_lag1', 'tempClosed_lag2', 'tempClosed_lag3', 'tempClosed_lag4', 'tempClosed_lag5',\n",
    "        'distance_lag1', 'Gaussian_1', 'Gaussian_2', 'Gaussian_3']]\n",
    "\n",
    "categorical_columns = ['Day of Week'] \n",
    "numerical_columns = [col for col in X_df.columns if col not in categorical_columns]\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_columns),\n",
    "    ('cat', OneHotEncoder(), categorical_columns)\n",
    "])\n",
    "\n",
    "X = preprocessor.fit_transform(X_df)\n",
    "y = Data_final['tempClosed']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation and creation of plots\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(class_weight='balanced', max_iter=10000),\n",
    "    'Random Forest': RandomForestClassifier(criterion = 'entropy', max_features = 'sqrt', class_weight='balanced', random_state=42),\n",
    "}\n",
    "\n",
    "# Evaluate models\n",
    "for model_name, model in tqdm(models.items()):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    rs = recall_score(y_test, y_pred)\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[1, 0])\n",
    "\n",
    "    print(f'{model_name}:')\n",
    "    print(f'F1-score = {f1:.4f}')\n",
    "    print(f'Recall = {rs:.4f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(cm)\n",
    "    print('\\n')\n",
    "\n",
    "dump(models['Random Forest'], '/content/drive/My Drive/IDS560-SpotOnResponse/rf.joblib')\n",
    "\n",
    "# Calculate ROC curve and Precision-Recall curve for Logistic Regression\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, models['Logistic Regression'].predict_proba(X_test)[:,1])\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, models['Logistic Regression'].predict_proba(X_test)[:,1])\n",
    "\n",
    "# Calculate ROC curve and Precision-Recall curve for Random Forest\n",
    "rf_fpr, rf_tpr, _ = roc_curve(y_test, models['Random Forest'].predict_proba(X_test)[:,1])\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_test, models['Random Forest'].predict_proba(X_test)[:,1])\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure()\n",
    "plt.plot(lr_fpr, lr_tpr, label='Logistic Regression')\n",
    "plt.plot(rf_fpr, rf_tpr, label='Random Forest')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot Precision-Recall curve\n",
    "plt.figure()\n",
    "plt.plot(lr_recall, lr_precision, label='Logistic Regression')\n",
    "plt.plot(rf_recall, rf_precision, label='Random Forest')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature and permutation importance\n",
    "# Get feature importances\n",
    "feature_importances = models[\"Random Forest\"].feature_importances_\n",
    "\n",
    "# Sort feature importances in descending order\n",
    "sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "# Print feature importances\n",
    "print(\"Feature Importances:\")\n",
    "for i, idx in enumerate(sorted_indices):\n",
    "    print(f\"{i + 1}. Feature {idx}: {feature_importances[idx]}\")\n",
    "\n",
    "# Calculate permutation importances using recall as the evaluation metric\n",
    "perm_importance = permutation_importance(models[\"Random Forest\"], X_test, y_test, scoring='recall')\n",
    "\n",
    "# Sort permutation importances in descending order\n",
    "sorted_perm_indices = np.argsort(perm_importance.importances_mean)[::-1]\n",
    "\n",
    "# Print permutation importances\n",
    "print(\"\\nPermutation Importances (using recall as the evaluation metric):\")\n",
    "for i, idx in enumerate(sorted_perm_indices):\n",
    "    print(f\"{i + 1}. Feature {idx}: {perm_importance.importances_mean[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistics Regression grid search\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score, f1_score, roc_auc_score, log_loss\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'penalty': [\"l1\", \"l2\"],\n",
    "    'C' : [.01, .1, 0, 1, 10]\n",
    "}\n",
    "\n",
    "# Define scoring functions for recall, F1-score, ROC-AUC\n",
    "scoring = {\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "}\n",
    "\n",
    "# Initialize a Logistic Regression classifier\n",
    "logreg_classifier = LogisticRegression(solver='liblinear', random_state=42, max_iter=10000, class_weight='balanced')\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=logreg_classifier, param_grid=param_grid, cv=StratifiedKFold(n_splits=4, shuffle=True, random_state=42),\n",
    "                           scoring=scoring, refit=False)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Extract mean scores from the grid search results\n",
    "mean_recall = grid_search.cv_results_['mean_test_recall']\n",
    "mean_f1 = grid_search.cv_results_['mean_test_f1']\n",
    "mean_roc_auc = grid_search.cv_results_['mean_test_roc_auc']\n",
    "\n",
    "# Output the mean scores for each parameter combination\n",
    "for i, params in enumerate(grid_search.cv_results_['params']):\n",
    "    print(\"Parameter combination:\", params)\n",
    "    print(\"Mean Recall:\", mean_recall[i])\n",
    "    print(\"Mean F1-score:\", mean_f1[i])\n",
    "    print(\"Mean ROC-AUC:\", mean_roc_auc[i])\n",
    "\n",
    "#Random forest grid search\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer, recall_score, f1_score, roc_auc_score, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'criterion': [\"gini\", \"entropy\"],\n",
    "    'max_features': [\"sqrt\", \"log2\", None]\n",
    "}\n",
    "\n",
    "# Define scoring functions for recall, F1-score, ROC-AUC\n",
    "scoring = {\n",
    "    'recall': make_scorer(recall_score),\n",
    "    'f1': make_scorer(f1_score),\n",
    "    'roc_auc': make_scorer(roc_auc_score),\n",
    "}\n",
    "\n",
    "# Initialize a Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=StratifiedKFold(n_splits=4, shuffle=True, random_state=42),\n",
    "                           scoring=scoring, refit=False)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Extract mean scores from the grid search results\n",
    "mean_recall = grid_search.cv_results_['mean_test_recall']\n",
    "mean_f1 = grid_search.cv_results_['mean_test_f1']\n",
    "mean_roc_auc = grid_search.cv_results_['mean_test_roc_auc']\n",
    "\n",
    "# Output the mean scores for each parameter combination\n",
    "for i, params in enumerate(grid_search.cv_results_['params']):\n",
    "    print(\"Parameter combination:\", params)\n",
    "    print(\"Mean Recall:\", mean_recall[i])\n",
    "    print(\"Mean F1-score:\", mean_f1[i])\n",
    "    print(\"Mean ROC-AUC:\", mean_roc_auc[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
